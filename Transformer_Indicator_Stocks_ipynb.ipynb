{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1HrjLPl8SvFyBGCu4gGb6I76fnO-FhpFx",
      "authorship_tag": "ABX9TyNae5lrHis6/rnrPWUvDvfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadashkarimi/CPSC/blob/master/Transformer_Indicator_Stocks_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "drz_VIWNty8R",
        "outputId": "2d6050f5-8913-4423-a97c-c99f3851f690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data for cryptocurrencies and stocks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-a9a4142c9375>:253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data['Class'] = classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from drive/MyDrive/Transformer/bitcoin_transformer_indicator_model.pth...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-a9a4142c9375>:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(file_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a9a4142c9375>\u001b[0m in \u001b[0;36m<cell line: 257>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model from {model_file}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No saved model found. Starting fresh.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-a9a4142c9375>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(file_path, model, optimizer)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1361\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   1813\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0m_internal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \"\"\"\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mbackend_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_privateuse1_backend_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mdevice_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_available\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdevice_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    509\u001b[0m             \u001b[0;34mf\"Attempting to deserialize object on a {backend_name.upper()} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;34mf\"device but torch.{backend_name}.is_available() is False. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Define indicator functions\n",
        "\n",
        "def compute_RSI(data, window=14):\n",
        "    delta = data.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).fillna(0)\n",
        "    loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
        "    avg_gain = gain.rolling(window=window).mean()\n",
        "    avg_loss = loss.rolling(window=window).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "def EMA(data, period=14):\n",
        "    return data.ewm(span=period, adjust=False).mean()\n",
        "\n",
        "def SMA(data, period=14):\n",
        "    return data.rolling(window=period).mean()\n",
        "\n",
        "def BollingerBands(data, period=20, std_dev=2):\n",
        "    sma = SMA(data, period)\n",
        "    std = data.rolling(window=period).std()\n",
        "    upper_band = sma + (std_dev * std)\n",
        "    lower_band = sma - (std_dev * std)\n",
        "    return upper_band, lower_band\n",
        "\n",
        "def MACD(data, short_period=12, long_period=26, signal_period=9):\n",
        "    short_ema = EMA(data, short_period)\n",
        "    long_ema = EMA(data, long_period)\n",
        "    macd = short_ema - long_ema\n",
        "    signal = EMA(macd, signal_period)\n",
        "    histogram = macd - signal\n",
        "    return macd, signal, histogram\n",
        "\n",
        "def ATR(high, low, close, period=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift(1))\n",
        "    tr3 = abs(low - close.shift(1))\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=period).mean()\n",
        "\n",
        "def StochasticOscillator(close, high, low, period=14):\n",
        "    L14 = low.rolling(window=period).min()\n",
        "    H14 = high.rolling(window=period).max()\n",
        "    K = 100 * ((close - L14) / (H14 - L14))\n",
        "    D = K.rolling(window=3).mean()\n",
        "    return K, D\n",
        "\n",
        "def CCI(high, low, close, period=20):\n",
        "    TP = (high + low + close) / 3\n",
        "    sma = SMA(TP, period)\n",
        "    mad = (TP - sma).abs().rolling(window=period).mean()\n",
        "    return (TP - sma) / (0.015 * mad)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, file_path):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'loss': loss\n",
        "    }, file_path)\n",
        "\n",
        "def load_checkpoint(file_path, model, optimizer):\n",
        "    checkpoint = torch.load(file_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return model, optimizer, epoch, loss\n",
        "\n",
        "def ParabolicSAR(high, low, acceleration=0.02, max_acceleration=0.2):\n",
        "    # Ensure high and low are Pandas Series\n",
        "    if not isinstance(high, pd.Series) or not isinstance(low, pd.Series):\n",
        "        raise TypeError(\"High and Low inputs must be Pandas Series.\")\n",
        "\n",
        "    # Check if columns are empty or contain all null values\n",
        "    if high.isnull().all():\n",
        "        raise ValueError(\"High column is empty or contains all null values.\")\n",
        "    if low.isnull().all():\n",
        "        raise ValueError(\"Low column is empty or contains all null values.\")\n",
        "\n",
        "    # Handle NaN values\n",
        "    high = high.fillna(method='ffill').fillna(method='bfill')\n",
        "    low = low.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    sar = low.iloc[0]\n",
        "    ep = high.iloc[0]  # Extreme point\n",
        "    af = acceleration\n",
        "    psar = [sar]\n",
        "\n",
        "    for i in range(1, len(high)):\n",
        "        new_sar = sar + af * (ep - sar)\n",
        "\n",
        "        # Check for trend reversal\n",
        "        if new_sar > high.iloc[i]:\n",
        "            new_sar = high.iloc[i - 1]\n",
        "            ep = low.iloc[i]\n",
        "            af = acceleration\n",
        "        elif new_sar < low.iloc[i]:\n",
        "            new_sar = low.iloc[i - 1]\n",
        "            ep = high.iloc[i]\n",
        "            af = acceleration\n",
        "        else:\n",
        "            # Update extreme point and acceleration factor\n",
        "            if high.iloc[i] > ep:\n",
        "                ep = high.iloc[i]\n",
        "                af = min(af + acceleration, max_acceleration)\n",
        "            elif low.iloc[i] < ep:\n",
        "                ep = low.iloc[i]\n",
        "                af = min(af + acceleration, max_acceleration)\n",
        "\n",
        "        psar.append(new_sar)\n",
        "        sar = new_sar\n",
        "\n",
        "    return pd.Series(psar, index=high.index)\n",
        "\n",
        "\n",
        "\n",
        "def Ichimoku(high, low, period1=9, period2=26, period3=52):\n",
        "    conversion = (high.rolling(period1).max() + low.rolling(period1).min()) / 2\n",
        "    base = (high.rolling(period2).max() + low.rolling(period2).min()) / 2\n",
        "    span_a = ((conversion + base) / 2).shift(period2)\n",
        "    span_b = ((high.rolling(period3).max() + low.rolling(period3).min()) / 2).shift(period2)\n",
        "    return conversion, base, span_a, span_b\n",
        "\n",
        "def VWAP(close, volume):\n",
        "    return (close * volume).cumsum() / volume.cumsum()\n",
        "\n",
        "def Aroon(high, low, period=25):\n",
        "    aroon_up = 100 * (period - high.rolling(period).apply(lambda x: x.argmax())) / period\n",
        "    aroon_down = 100 * (period - low.rolling(period).apply(lambda x: x.argmin())) / period\n",
        "    return aroon_up, aroon_down\n",
        "\n",
        "def CMF(high, low, close, volume, period=20):\n",
        "    money_flow_multiplier = ((close - low) - (high - close)) / (high - low)\n",
        "    money_flow_volume = money_flow_multiplier * volume\n",
        "    return money_flow_volume.rolling(window=period).sum() / volume.rolling(window=period).sum()\n",
        "\n",
        "def WilliamsR(high, low, close, period=14):\n",
        "    return ((high.rolling(period).max() - close) / (high.rolling(period).max() - low.rolling(period).min())) * -100\n",
        "\n",
        "def ROC(data, period=12):\n",
        "    return ((data - data.shift(period)) / data.shift(period)) * 100\n",
        "\n",
        "def Momentum(data, period=14):\n",
        "    return data - data.shift(period)\n",
        "\n",
        "def FibonacciRetracement(high, low):\n",
        "    diff = high - low\n",
        "    return [high - diff * level for level in [0.236, 0.382, 0.5, 0.618, 0.786]]\n",
        "\n",
        "def compute_indicators(data):\n",
        "    required_columns = ['High', 'Low', 'Close', 'Volume']\n",
        "    for col in required_columns:\n",
        "        if col not in data:\n",
        "            raise KeyError(f\"Column '{col}' is missing from data.\")\n",
        "\n",
        "    indicators = {}\n",
        "    indicators['RSI'] = compute_RSI(data['Close'])\n",
        "    indicators['EMA_20'] = EMA(data['Close'], period=20)\n",
        "    indicators['Bollinger_Upper'], indicators['Bollinger_Lower'] = BollingerBands(data['Close'], period=20, std_dev=2)\n",
        "    macd, signal, histogram = MACD(data['Close'])\n",
        "    indicators['MACD'] = macd\n",
        "    indicators['Signal'] = signal\n",
        "    indicators['Histogram'] = histogram\n",
        "    indicators['Stochastic_K'], indicators['Stochastic_D'] = StochasticOscillator(data['Close'], data['High'], data['Low'])\n",
        "    indicators['ATR'] = ATR(data['High'], data['Low'], data['Close'])\n",
        "    indicators['CCI'] = CCI(data['High'], data['Low'], data['Close'])\n",
        "    # indicators['Parabolic_SAR'] = ParabolicSAR(data['High'], data['Low'])\n",
        "    indicators['VWAP'] = VWAP(data['Close'], data['Volume'])\n",
        "    indicators['Aroon_Up'], indicators['Aroon_Down'] = Aroon(data['High'], data['Low'])\n",
        "    indicators['CMF'] = CMF(data['High'], data['Low'], data['Close'], data['Volume'])\n",
        "    indicators['Williams_R'] = WilliamsR(data['High'], data['Low'], data['Close'])\n",
        "    indicators['ROC'] = ROC(data['Close'])\n",
        "    indicators['Momentum'] = Momentum(data['Close'])\n",
        "    return indicators\n",
        "\n",
        "class BitcoinDataset(Dataset):\n",
        "    def __init__(self, data, seq_len, n_days):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "        self.n_days = n_days\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len - self.n_days + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_len, :-1].astype(np.float32)\n",
        "        y = self.data[idx + self.seq_len: idx + self.seq_len + self.n_days, -1].astype(np.int64)\n",
        "        return x, y\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, n_days, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 1000, d_model))\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes * n_days)\n",
        "        self.n_days = n_days\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = self.input_projection(x) + self.positional_encoding[:, :seq_len, :]\n",
        "        transformer_out = self.transformer(x, x)\n",
        "        out = self.fc(transformer_out[:, -1, :])\n",
        "        return out.view(-1, self.n_days, self.num_classes)\n",
        "\n",
        "def preprocess_data(data_dict, target_symbol, bins):\n",
        "    for symbol in data_dict.keys():\n",
        "        stock_data = data_dict[symbol]\n",
        "        indicators = compute_indicators(stock_data)\n",
        "        for key, value in indicators.items():\n",
        "            stock_data[key] = value\n",
        "\n",
        "    combined_df = pd.concat(\n",
        "        [data[['Close', 'RSI', 'EMA_20', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'Signal', 'Histogram', 'Stochastic_K', 'Stochastic_D', 'ATR', 'CCI', 'VWAP', 'Aroon_Up', 'Aroon_Down', 'CMF', 'Williams_R', 'ROC', 'Momentum']]\n",
        "         for data in data_dict.values()], axis=1, keys=data_dict.keys())\n",
        "\n",
        "    combined_df.columns = ['{}_{}'.format(symbol, col) for symbol in data_dict.keys()\n",
        "                           for col in ['Close', 'RSI', 'EMA_20', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'Signal', 'Histogram', 'Stochastic_K', 'Stochastic_D', 'ATR', 'CCI', 'VWAP', 'Aroon_Up', 'Aroon_Down', 'CMF', 'Williams_R', 'ROC', 'Momentum']]\n",
        "\n",
        "    combined_df = assign_classes(combined_df, f'{target_symbol}_Close', bins)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_features = scaler.fit_transform(combined_df.iloc[:, :-1])\n",
        "\n",
        "    combined_data = np.hstack((scaled_features, combined_df['Class'].values.reshape(-1, 1)))\n",
        "    return combined_data, list(combined_df.columns), scaler\n",
        "\n",
        "def assign_classes(data, price_column, bins):\n",
        "    pct_change = (data[price_column].diff() / data[price_column].shift(1)) * 100\n",
        "    classes = np.digitize(pct_change, bins) - 3\n",
        "    classes = classes + 2\n",
        "    data['Class'] = classes\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    symbols = [\n",
        "        \"BTC-USD\", \"AAPL\", \"MSFT\", \"NVDA\", \"TSLA\", \"GOOGL\", \"AMZN\", \"META\", \"BRK-B\", \"JPM\",\n",
        "        \"V\", \"PG\", \"UNH\", \"HD\", \"MA\", \"DIS\", \"ADBE\", \"CRM\", \"PYPL\", \"NFLX\",\n",
        "        \"INTC\", \"PEP\", \"KO\", \"NKE\", \"MCD\", \"WMT\", \"CSCO\", \"QCOM\", \"ORCL\", \"ABBV\",\n",
        "        \"T\", \"XOM\", \"CVX\", \"VZ\", \"PFE\", \"MRK\", \"LLY\", \"BMY\", \"COST\", \"DHR\",\n",
        "        \"TMO\", \"HON\", \"IBM\", \"AMD\", \"BA\", \"CAT\", \"SPGI\", \"GS\", \"MS\", \"GOLD\"\n",
        "    ]\n",
        "    target_symbol = \"BTC-USD\"\n",
        "    model_file = \"drive/MyDrive/Transformer/bitcoin_transformer_indicator_model.pth\"\n",
        "    seq_len = 30\n",
        "    hidden_dim = 128\n",
        "    num_layers = 5\n",
        "    num_classes = 5\n",
        "    n_days = 7\n",
        "    batch_size = 32\n",
        "    learning_rate = 1e-4\n",
        "    num_epochs = 3000\n",
        "\n",
        "    print(\"Fetching data for cryptocurrencies and stocks...\")\n",
        "    stock_data = {symbol: yf.download(symbol, start=\"2020-01-01\", end=datetime.today().strftime('%Y-%m-%d')) for symbol in symbols}\n",
        "\n",
        "    bins = [-np.inf, -2, -0.5, 0.5, 2, np.inf]\n",
        "\n",
        "    print(\"Preprocessing data...\")\n",
        "    data, features, scaler = preprocess_data(stock_data, target_symbol, bins)\n",
        "\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n",
        "\n",
        "    train_dataset = BitcoinDataset(train_data, seq_len=seq_len, n_days=n_days)\n",
        "    test_dataset = BitcoinDataset(test_data, seq_len=seq_len, n_days=n_days)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = TransformerModel(\n",
        "        input_dim=len(features) - 1,  # Updated for additional indicators\n",
        "        num_classes=num_classes,\n",
        "        n_days=n_days,\n",
        "        d_model=128,\n",
        "        nhead=4,\n",
        "        num_layers=num_layers,\n",
        "        dim_feedforward=256\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if os.path.exists(model_file):\n",
        "        print(f\"Loading model from {model_file}...\")\n",
        "        model, optimizer, start_epoch, best_loss = load_checkpoint(model_file, model, optimizer)\n",
        "    else:\n",
        "        print(\"No saved model found. Starting fresh.\")\n",
        "        start_epoch = 0\n",
        "        best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x)\n",
        "            loss = 0\n",
        "            for day in range(n_days):\n",
        "                loss += criterion(outputs[:, day, :], y[:, day])\n",
        "            loss /= n_days\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            save_checkpoint(model, optimizer, epoch, best_loss, model_file)\n",
        "\n",
        "    print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available! GPU count: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "id": "yYUHAS3gvMQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afae507-0df5-4c2c-c745-716a8dd6c45d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aQIuMKLevhtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime, timedelta\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Predict future trends using the trained model\n",
        "print(\"Predicting future trends...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Use the last sequence from the dataset for future predictions\n",
        "    recent_data = torch.tensor(data[-seq_len:, :-1].astype(np.float32)).unsqueeze(0).to(device)\n",
        "    outputs = model(recent_data)  # Shape: [1, n_days, num_classes]\n",
        "    predicted_classes = torch.argmax(outputs, dim=2).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Map predictions back to percentage changes\n",
        "    class_to_pct_change = {0: -5, 1: -2, 2: 0, 3: 2, 4: 5}\n",
        "    predicted_pct_changes = [class_to_pct_change[p] for p in predicted_classes]\n",
        "\n",
        "    # Simulate future prices based on recent price and predicted percentage changes\n",
        "    recent_prices = scaler.inverse_transform(data[-seq_len:, :-1])[:, 0]  # Reverse scaling for the target feature\n",
        "    last_price = recent_prices[-1]\n",
        "    future_prices = [last_price]\n",
        "\n",
        "    for pct_change in predicted_pct_changes:\n",
        "        next_price = future_prices[-1] * (1 + pct_change / 100)\n",
        "        future_prices.append(next_price)\n",
        "\n",
        "    future_prices = np.array(future_prices[1:])  # Remove the first value (duplicate of last_price)\n",
        "\n",
        "# Generate dates for recent and future prices\n",
        "seq_len = len(recent_prices)\n",
        "n_days = len(predicted_pct_changes)\n",
        "today = datetime.today()\n",
        "recent_dates = [today - timedelta(days=seq_len - i) for i in range(seq_len)]\n",
        "future_dates = [recent_dates[-1] + timedelta(days=i + 1) for i in range(n_days)]\n",
        "\n",
        "# Enhanced visualization\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(recent_dates, recent_prices, label=\"Recent Prices\", color=\"#1f77b4\", linewidth=2)\n",
        "plt.plot(future_dates, future_prices, label=\"Predicted Prices\", color=\"#ff7f0e\", linestyle=\"--\", linewidth=2)\n",
        "\n",
        "# Highlight today's date\n",
        "today_price = recent_prices[-1]\n",
        "plt.scatter([today], [today_price], color=\"red\", label=\"Today\", zorder=5)\n",
        "plt.text(today, today_price, f\" {today_price:.2f} USD\", fontsize=10, color=\"red\", weight=\"bold\")\n",
        "\n",
        "# Add grid and format\n",
        "plt.grid(which='major', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format x-axis as \"Month Day\"\n",
        "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))    # Show ticks for every day\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Bitcoin Price Prediction\", fontsize=16, weight='bold', pad=20)\n",
        "plt.xlabel(\"Date\", fontsize=12, labelpad=10)\n",
        "plt.ylabel(\"Price (USD)\", fontsize=12, labelpad=10)\n",
        "\n",
        "# Add legend\n",
        "plt.legend(fontsize=10, loc=\"upper left\")\n",
        "\n",
        "# Add shaded region for the prediction\n",
        "plt.axvspan(future_dates[0], future_dates[-1], color=\"orange\", alpha=0.1, label=\"Prediction Period\")\n",
        "\n",
        "# Final layout adjustments and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IEeSktMawj3P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "e61f9803-9a94-450a-bd4f-ef4abc46536d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting future trends...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-485c02f9456f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Predict future trends using the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicting future trends...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Use the last sequence from the dataset for future predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIyQq2kvD9a-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}