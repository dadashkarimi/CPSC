{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1HrjLPl8SvFyBGCu4gGb6I76fnO-FhpFx",
      "authorship_tag": "ABX9TyNt/ibNaen7N7nD5vY/F1IF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadashkarimi/CPSC/blob/master/Transformer_Indicator_Stocks_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "drz_VIWNty8R",
        "outputId": "b1b63640-d749-4d1d-90ac-88de9ab7953d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data for cryptocurrencies and stocks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-5d9722d9c929>:253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data['Class'] = classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved model found. Starting fresh.\n",
            "Epoch [1/100] Loss: 1.6452\n",
            "Epoch [2/100] Loss: 1.6204\n",
            "Epoch [3/100] Loss: 1.6163\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5d9722d9c929>\u001b[0m in \u001b[0;36m<cell line: 257>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mn_days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Define indicator functions\n",
        "\n",
        "def compute_RSI(data, window=14):\n",
        "    delta = data.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).fillna(0)\n",
        "    loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
        "    avg_gain = gain.rolling(window=window).mean()\n",
        "    avg_loss = loss.rolling(window=window).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "def EMA(data, period=14):\n",
        "    return data.ewm(span=period, adjust=False).mean()\n",
        "\n",
        "def SMA(data, period=14):\n",
        "    return data.rolling(window=period).mean()\n",
        "\n",
        "def BollingerBands(data, period=20, std_dev=2):\n",
        "    sma = SMA(data, period)\n",
        "    std = data.rolling(window=period).std()\n",
        "    upper_band = sma + (std_dev * std)\n",
        "    lower_band = sma - (std_dev * std)\n",
        "    return upper_band, lower_band\n",
        "\n",
        "def MACD(data, short_period=12, long_period=26, signal_period=9):\n",
        "    short_ema = EMA(data, short_period)\n",
        "    long_ema = EMA(data, long_period)\n",
        "    macd = short_ema - long_ema\n",
        "    signal = EMA(macd, signal_period)\n",
        "    histogram = macd - signal\n",
        "    return macd, signal, histogram\n",
        "\n",
        "def ATR(high, low, close, period=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift(1))\n",
        "    tr3 = abs(low - close.shift(1))\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=period).mean()\n",
        "\n",
        "def StochasticOscillator(close, high, low, period=14):\n",
        "    L14 = low.rolling(window=period).min()\n",
        "    H14 = high.rolling(window=period).max()\n",
        "    K = 100 * ((close - L14) / (H14 - L14))\n",
        "    D = K.rolling(window=3).mean()\n",
        "    return K, D\n",
        "\n",
        "def CCI(high, low, close, period=20):\n",
        "    TP = (high + low + close) / 3\n",
        "    sma = SMA(TP, period)\n",
        "    mad = (TP - sma).abs().rolling(window=period).mean()\n",
        "    return (TP - sma) / (0.015 * mad)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, file_path):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'loss': loss\n",
        "    }, file_path)\n",
        "\n",
        "def load_checkpoint(file_path, model, optimizer):\n",
        "    checkpoint = torch.load(file_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return model, optimizer, epoch, loss\n",
        "\n",
        "def ParabolicSAR(high, low, acceleration=0.02, max_acceleration=0.2):\n",
        "    # Ensure high and low are Pandas Series\n",
        "    if not isinstance(high, pd.Series) or not isinstance(low, pd.Series):\n",
        "        raise TypeError(\"High and Low inputs must be Pandas Series.\")\n",
        "\n",
        "    # Check if columns are empty or contain all null values\n",
        "    if high.isnull().all():\n",
        "        raise ValueError(\"High column is empty or contains all null values.\")\n",
        "    if low.isnull().all():\n",
        "        raise ValueError(\"Low column is empty or contains all null values.\")\n",
        "\n",
        "    # Handle NaN values\n",
        "    high = high.fillna(method='ffill').fillna(method='bfill')\n",
        "    low = low.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    sar = low.iloc[0]\n",
        "    ep = high.iloc[0]  # Extreme point\n",
        "    af = acceleration\n",
        "    psar = [sar]\n",
        "\n",
        "    for i in range(1, len(high)):\n",
        "        new_sar = sar + af * (ep - sar)\n",
        "\n",
        "        # Check for trend reversal\n",
        "        if new_sar > high.iloc[i]:\n",
        "            new_sar = high.iloc[i - 1]\n",
        "            ep = low.iloc[i]\n",
        "            af = acceleration\n",
        "        elif new_sar < low.iloc[i]:\n",
        "            new_sar = low.iloc[i - 1]\n",
        "            ep = high.iloc[i]\n",
        "            af = acceleration\n",
        "        else:\n",
        "            # Update extreme point and acceleration factor\n",
        "            if high.iloc[i] > ep:\n",
        "                ep = high.iloc[i]\n",
        "                af = min(af + acceleration, max_acceleration)\n",
        "            elif low.iloc[i] < ep:\n",
        "                ep = low.iloc[i]\n",
        "                af = min(af + acceleration, max_acceleration)\n",
        "\n",
        "        psar.append(new_sar)\n",
        "        sar = new_sar\n",
        "\n",
        "    return pd.Series(psar, index=high.index)\n",
        "\n",
        "\n",
        "\n",
        "def Ichimoku(high, low, period1=9, period2=26, period3=52):\n",
        "    conversion = (high.rolling(period1).max() + low.rolling(period1).min()) / 2\n",
        "    base = (high.rolling(period2).max() + low.rolling(period2).min()) / 2\n",
        "    span_a = ((conversion + base) / 2).shift(period2)\n",
        "    span_b = ((high.rolling(period3).max() + low.rolling(period3).min()) / 2).shift(period2)\n",
        "    return conversion, base, span_a, span_b\n",
        "\n",
        "def VWAP(close, volume):\n",
        "    return (close * volume).cumsum() / volume.cumsum()\n",
        "\n",
        "def Aroon(high, low, period=25):\n",
        "    aroon_up = 100 * (period - high.rolling(period).apply(lambda x: x.argmax())) / period\n",
        "    aroon_down = 100 * (period - low.rolling(period).apply(lambda x: x.argmin())) / period\n",
        "    return aroon_up, aroon_down\n",
        "\n",
        "def CMF(high, low, close, volume, period=20):\n",
        "    money_flow_multiplier = ((close - low) - (high - close)) / (high - low)\n",
        "    money_flow_volume = money_flow_multiplier * volume\n",
        "    return money_flow_volume.rolling(window=period).sum() / volume.rolling(window=period).sum()\n",
        "\n",
        "def WilliamsR(high, low, close, period=14):\n",
        "    return ((high.rolling(period).max() - close) / (high.rolling(period).max() - low.rolling(period).min())) * -100\n",
        "\n",
        "def ROC(data, period=12):\n",
        "    return ((data - data.shift(period)) / data.shift(period)) * 100\n",
        "\n",
        "def Momentum(data, period=14):\n",
        "    return data - data.shift(period)\n",
        "\n",
        "def FibonacciRetracement(high, low):\n",
        "    diff = high - low\n",
        "    return [high - diff * level for level in [0.236, 0.382, 0.5, 0.618, 0.786]]\n",
        "\n",
        "def compute_indicators(data):\n",
        "    required_columns = ['High', 'Low', 'Close', 'Volume']\n",
        "    for col in required_columns:\n",
        "        if col not in data:\n",
        "            raise KeyError(f\"Column '{col}' is missing from data.\")\n",
        "\n",
        "    indicators = {}\n",
        "    indicators['RSI'] = compute_RSI(data['Close'])\n",
        "    indicators['EMA_20'] = EMA(data['Close'], period=20)\n",
        "    indicators['Bollinger_Upper'], indicators['Bollinger_Lower'] = BollingerBands(data['Close'], period=20, std_dev=2)\n",
        "    macd, signal, histogram = MACD(data['Close'])\n",
        "    indicators['MACD'] = macd\n",
        "    indicators['Signal'] = signal\n",
        "    indicators['Histogram'] = histogram\n",
        "    indicators['Stochastic_K'], indicators['Stochastic_D'] = StochasticOscillator(data['Close'], data['High'], data['Low'])\n",
        "    indicators['ATR'] = ATR(data['High'], data['Low'], data['Close'])\n",
        "    indicators['CCI'] = CCI(data['High'], data['Low'], data['Close'])\n",
        "    # indicators['Parabolic_SAR'] = ParabolicSAR(data['High'], data['Low'])\n",
        "    indicators['VWAP'] = VWAP(data['Close'], data['Volume'])\n",
        "    indicators['Aroon_Up'], indicators['Aroon_Down'] = Aroon(data['High'], data['Low'])\n",
        "    indicators['CMF'] = CMF(data['High'], data['Low'], data['Close'], data['Volume'])\n",
        "    indicators['Williams_R'] = WilliamsR(data['High'], data['Low'], data['Close'])\n",
        "    indicators['ROC'] = ROC(data['Close'])\n",
        "    indicators['Momentum'] = Momentum(data['Close'])\n",
        "    return indicators\n",
        "\n",
        "class BitcoinDataset(Dataset):\n",
        "    def __init__(self, data, seq_len, n_days):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "        self.n_days = n_days\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len - self.n_days + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_len, :-1].astype(np.float32)\n",
        "        y = self.data[idx + self.seq_len: idx + self.seq_len + self.n_days, -1].astype(np.int64)\n",
        "        return x, y\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, n_days, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 1000, d_model))\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes * n_days)\n",
        "        self.n_days = n_days\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = self.input_projection(x) + self.positional_encoding[:, :seq_len, :]\n",
        "        transformer_out = self.transformer(x, x)\n",
        "        out = self.fc(transformer_out[:, -1, :])\n",
        "        return out.view(-1, self.n_days, self.num_classes)\n",
        "\n",
        "def preprocess_data(data_dict, target_symbol, bins):\n",
        "    for symbol in data_dict.keys():\n",
        "        stock_data = data_dict[symbol]\n",
        "        indicators = compute_indicators(stock_data)\n",
        "        for key, value in indicators.items():\n",
        "            stock_data[key] = value\n",
        "\n",
        "    combined_df = pd.concat(\n",
        "        [data[['Close', 'RSI', 'EMA_20', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'Signal', 'Histogram', 'Stochastic_K', 'Stochastic_D', 'ATR', 'CCI', 'VWAP', 'Aroon_Up', 'Aroon_Down', 'CMF', 'Williams_R', 'ROC', 'Momentum']]\n",
        "         for data in data_dict.values()], axis=1, keys=data_dict.keys())\n",
        "\n",
        "    combined_df.columns = ['{}_{}'.format(symbol, col) for symbol in data_dict.keys()\n",
        "                           for col in ['Close', 'RSI', 'EMA_20', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'Signal', 'Histogram', 'Stochastic_K', 'Stochastic_D', 'ATR', 'CCI', 'VWAP', 'Aroon_Up', 'Aroon_Down', 'CMF', 'Williams_R', 'ROC', 'Momentum']]\n",
        "\n",
        "    combined_df = assign_classes(combined_df, f'{target_symbol}_Close', bins)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_features = scaler.fit_transform(combined_df.iloc[:, :-1])\n",
        "\n",
        "    combined_data = np.hstack((scaled_features, combined_df['Class'].values.reshape(-1, 1)))\n",
        "    return combined_data, list(combined_df.columns), scaler\n",
        "\n",
        "def assign_classes(data, price_column, bins):\n",
        "    pct_change = (data[price_column].diff() / data[price_column].shift(1)) * 100\n",
        "    classes = np.digitize(pct_change, bins) - 3\n",
        "    classes = classes + 2\n",
        "    data['Class'] = classes\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    symbols = [\n",
        "        \"BTC-USD\", \"AAPL\", \"MSFT\", \"NVDA\", \"TSLA\", \"GOOGL\", \"AMZN\", \"META\", \"BRK-B\", \"JPM\",\n",
        "        \"V\", \"PG\", \"UNH\", \"HD\", \"MA\", \"DIS\", \"ADBE\", \"CRM\", \"PYPL\", \"NFLX\",\n",
        "        \"INTC\", \"PEP\", \"KO\", \"NKE\", \"MCD\", \"WMT\", \"CSCO\", \"QCOM\", \"ORCL\", \"ABBV\",\n",
        "        \"T\", \"XOM\", \"CVX\", \"VZ\", \"PFE\", \"MRK\", \"LLY\", \"BMY\", \"COST\", \"DHR\",\n",
        "        \"TMO\", \"HON\", \"IBM\", \"AMD\", \"BA\", \"CAT\", \"SPGI\", \"GS\", \"MS\", \"GOLD\"\n",
        "    ]\n",
        "    target_symbol = \"BTC-USD\"\n",
        "    model_file = \"drive/MyDrive/Transformer/bitcoin_transformer_indicator_model.pth\"\n",
        "    seq_len = 30\n",
        "    hidden_dim = 128\n",
        "    num_layers = 5\n",
        "    num_classes = 5\n",
        "    n_days = 7\n",
        "    batch_size = 32\n",
        "    learning_rate = 1e-4\n",
        "    num_epochs = 100\n",
        "\n",
        "    print(\"Fetching data for cryptocurrencies and stocks...\")\n",
        "    stock_data = {symbol: yf.download(symbol, start=\"2020-01-01\", end=datetime.today().strftime('%Y-%m-%d')) for symbol in symbols}\n",
        "\n",
        "    bins = [-np.inf, -2, -0.5, 0.5, 2, np.inf]\n",
        "\n",
        "    print(\"Preprocessing data...\")\n",
        "    data, features, scaler = preprocess_data(stock_data, target_symbol, bins)\n",
        "\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n",
        "\n",
        "    train_dataset = BitcoinDataset(train_data, seq_len=seq_len, n_days=n_days)\n",
        "    test_dataset = BitcoinDataset(test_data, seq_len=seq_len, n_days=n_days)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = TransformerModel(\n",
        "        input_dim=len(features) - 1,  # Updated for additional indicators\n",
        "        num_classes=num_classes,\n",
        "        n_days=n_days,\n",
        "        d_model=128,\n",
        "        nhead=4,\n",
        "        num_layers=num_layers,\n",
        "        dim_feedforward=256\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if os.path.exists(model_file):\n",
        "        print(f\"Loading model from {model_file}...\")\n",
        "        model, optimizer, start_epoch, best_loss = load_checkpoint(model_file, model, optimizer)\n",
        "    else:\n",
        "        print(\"No saved model found. Starting fresh.\")\n",
        "        start_epoch = 0\n",
        "        best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x)\n",
        "            loss = 0\n",
        "            for day in range(n_days):\n",
        "                loss += criterion(outputs[:, day, :], y[:, day])\n",
        "            loss /= n_days\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            save_checkpoint(model, optimizer, epoch, best_loss, model_file)\n",
        "\n",
        "    print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available! GPU count: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "id": "yYUHAS3gvMQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790f9d57-be2b-4659-9109-57adeb7c0378"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEeSktMawj3P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}